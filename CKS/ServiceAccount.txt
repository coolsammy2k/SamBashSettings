Authentication: Service Account Example


# Create Service Account

$ kubectl create serviceaccount sa1

# Get Service account token

$ kubectl get -o yaml sa sa1 SA_SECRET="$(kubectl get sa sa1 -o jsonpath='{.secrets[0].name}')" 

$ kubectl get -o yaml secret "${SA_SECRET}" SA_TOKEN="$(kubectl get secret "${SA_SECRET}" -o jsonpath='{.data.token}' | base64 -d)"

# Send Request

$ kubectl "--token=${SA_TOKEN}" get nodes

$ kubectl config set-credentials sa1 "--token=${SA_TOKEN}"

$ kubectl config set-context sa1 --cluster demo-rbac --user sa1


 Roles and ClusterRoles

 apiVersion: rbac.authorization.k8s.io/v1
 kind: Role
 metadata:
   namespace: default
     name: role1
     rules:
     - apiGroups: ['*']
     resources: ['pods', 'pods/log'] verbs: ['get', 'list']
     - apiGroups: ['*']
       resources: ['configmaps']
         resourceNames: ['my-configmap']
           verbs: ['get', 'list']

 • Roles and ClusterRoles define a set of allowed actions on resources
 • Role is namespaced
 • Cannot include non-namespaces resources or non-resource URLs



  ClusterRoles


  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
      name: clusterRole1
      rules:
      - apiGroups: ['*']
      resources: ['nodes', ‘pods'] verbs: ['get', 'list']
      - nonResourceURLs: ['/api', '/healthz*']
        verbs: ['get', 'head']



ClusterRole is not namespaced
• non-namespaced resources access
• non-resource URLs access





 Authentication: X509 Client Cert, PKI Example
 • User: generate user private key (if not exist) 

$ openssl genrsa -out user1.key 2048

 • User: generate user CSR
 
$ openssl req -new -key user1.key -out user1.csr -subj "/CN=user1/O=group1/O=group2"

 • Admin: sign user client cert

$ openssl x509 -req -in user1.csr -CA cluster-ca.crt -CAkey cluster-ca.key \ -set_serial 101 -extensions client -days 365 -outform PEM -out user1.crt

 • User: use with kubectl via options or kubeconfig
$ kubectl --client-key=user1.key --client-certificate=user1.crt get nodes

$ kubectl config set-credentials user1 --client-key user1.key --client-certificate user1.crt --embed-certs kubectl config set-context user1 --cluster demo-rbac --user user1
$ kubectl --context=user1 get nodes
  
$ kubectl config use-context user1
$ kubectl config get-contexts
$ kubectl get nodes


 Authentication: X509 Client Cert, K8S CSR

 # User: generate user CSR
 
$ openssl req -new -key user2.key -out user2.csr -subj "/CN=user2/O=group1/O=group2"

pply -f - <<EOF
apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata:
  name: user2
  spec:
    request: $(cat user2.csr | base64 | tr -d '\n')
      usages: ['digital signature', 'key encipherment',
          'client auth']
EOF



$ kubectl certificate approve user2
$ kubectl certificate deny user2
$ kubectl get csr user2 –o jsonpath='{.status.certificate}' | \
    base64 --decode > user2.crt



 User: use with kubectl via options or kubeconfig

$ kubectl --client-key=user2.key --client-certificate=user2.crt get nodes
$ kubectl config set-credentials user2 --client-key user2.key --client-certificate user2.crt --embed-certs kubectl config set-context user2 --cluster demo-rbac --user user2

---




Pod with ServiceAccount uses Secrets
  Create new Namespace ns-secure and perform everything else in there
  Create ServiceAccount secret-manager
  Create Secret sec-a1 with any literal content of your choice
  Create Secret sec-a2 with any file content of your choice (like /etc/hosts)
  Create Pod secret-manager image nginx which uses the new SA
  Make Secret sec-a1 available as environment variable SEC_A1
  Mount Secret sec-a2 into the Pod read-only under /etc/sec-a2
  Verify your solution worked


  Solution
To solve this we’re logged into our controlplane node cks-controlplane .
1.
  alias k=kubectl
  k create ns ns-secure

2.
  k -n ns-secure create sa secret-manager
3.
  k -n ns-secure create secret generic sec-a1 --from-literal user=admin
4.
  k -n ns-secure create secret generic sec-a2 --from-file index=/etc/hosts

5. 6. 7.
  k -n ns-secure run secret-manager --image=nginx -oyaml --dry-run=client > pod.yaml
Now edit the yaml to:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: secret-manager
  name: secret-manager
  namespace: ns-secure
spec:
  volumes:
    - name: sec-a2
      secret:
        secretName: sec-a2
  serviceAccountName: secret-manager
  containers:
    - image: nginx
      name: secret-manager
      volumeMounts:
        - name: sec-a2
          mountPath: /etc/sec-a2
          readOnly: true
      env:
        - name: SEC_A1
          valueFrom:
            secretKeyRef:
              name: sec-a1
              key: user
  dnsPolicy: ClusterFirst
  restartPolicy: Always

8.
And to verify:
  k -f pod.yaml create
  k -n ns-secure exec secret-manager -- env | grep SEC
  k -n ns-secure exec secret-manager -- mount | grep sec
---

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: deploy1
  name: deploy1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: deploy1
  template:
    metadata:
      labels:
        app: deploy1
    spec:
      serviceAccountName: special
      containers:
      - image: nginx
        name: nginx

---

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  automountServiceAccountToken: false
  containers:
  - image: nginx
    name: pod1
---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: special
  namespace: default
automountServiceAccountToken: false

---

k rollout restart deploy deploy1
k exec deploy1-775d6566dc-bq757 -- mount | grep serviceaccount
k exec deploy1-775d6566dc-qwthl -- mount | grep serviceaccount


#How to find which role or clusterrole binded to a service account in Kubernetes?

kubectl get rolebindings,clusterrolebindings \
  --all-namespaces  \
    -o custom-columns='KIND:kind,NAMESPACE:metadata.namespace,NAME:metadata.name,SERVICE_ACCOUNTS:subjects[?(@.kind=="ServiceAccount")].name' | grep "<SERVICE_ACCOUNT_NAME>"


#Access API token from secret of default service account

APISERVER=$(kubectl config view --minify | grep server | cut -f 2- -d ":" | tr -d " ")
SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ')
TOKEN=$(kubectl describe secret $SECRET_NAME | grep -E '^token' | cut -f2 -d':' | tr -d " ")

curl $APISERVER/api --header "Authorization: Bearer $TOKEN" --insecure

# Using jsonpath

APISERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
SECRET_NAME=$(kubectl get serviceaccount default -o jsonpath='{.secrets[0].name}')
TOKEN=$(kubectl get secret $SECRET_NAME -o jsonpath='{.data.token}' | base64 --decode)

curl $APISERVER/api --header "Authorization: Bearer $TOKEN" --insecure


---


How to find which role or clusterrole binded to a service account in Kubernetes?

You could do something like:
kubectl get rolebindings,clusterrolebindings \
  --all-namespaces  \
  -o custom-columns='KIND:kind,NAMESPACE:metadata.namespace,NAME:metadata.name,SERVICE_ACCOUNTS:subjects[?(@.kind=="ServiceAccount")].name' | grep "<SERVICE_ACCOUNT_NAME>"


kubectl apply -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
EOF


Verify imagePullSecrets was added to pod spec
Now, when a new Pod is created in the current namespace and using the default ServiceAccount, the new Pod has its spec.imagePullSecrets field set automatically:
kubectl run nginx --image=nginx --restart=Never
kubectl get pod nginx -o=jsonpath='{.spec.imagePullSecrets[0].name}{"\n"}'


Get token for admin-user from

kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"
---

A pod has been created in the gamma namespace using a service account called cluster-view. This service account has been granted additional permissions as compared to the default service account and can view resources cluster-wide on this Kubernetes cluster. While these permissions are important for the application in this pod to work, the secret token is still mounted on this pod.

# Update the Pod to use the field automountServiceAccountToken: false
#Using this option makes sure that the service account token secret is not mounted in the pod at the location '/var/run/secrets/kubernetes.io/serviceaccount'

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: apps-cluster-dash
  name: apps-cluster-dash
  namespace: gamma
spec:
  containers:
  - image: nginx
    name: apps-cluster-dash
  serviceAccountName: cluster-view
  automountServiceAccountToken: false
